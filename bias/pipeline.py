"""
ä¸»Pipelineæ¨¡å—
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›å®Œæ•´çš„å®¡ç¨¿åå·®åˆ†ææµç¨‹

æ–°æµç¨‹ï¼ˆå››æ­¥éª¤ï¼‰:
1. ç‰¹å¾æå–: ä½¿ç”¨LLMæå–æ¯ä¸ªå®¡ç¨¿äººçš„ä¼˜ç¼ºç‚¹ -> è¾“å‡ºæ–‡ä»¶
2. åŒ¿ååŒ–å¤„ç†: å»é™¤å®¡ç¨¿äººä¿¡æ¯ï¼Œæ‰“ä¹±é¡ºåº -> è¾“å‡ºæ–°æ–‡ä»¶
3. æƒé‡é‡åŒ–: åŸºäºåŒ¿ååŒ–æ–‡ä»¶+PDFå†…å®¹ï¼Œä½¿ç”¨LLMé‡åŒ– -> è¾“å‡ºé‡åŒ–æ–‡ä»¶
4. åŒ¹é…è®¡ç®—: ä»£ç é€»è¾‘åŒ¹é…å›å®¡ç¨¿äººï¼Œçº¿æ€§ç›¸åŠ å¾—åˆ†æ•°
"""

from typing import List, Optional, Union
from pathlib import Path
import json
import time

from config import Config
from data_loader import DataLoader, Paper
from feature_extractor import FeatureExtractor
from llm_quantifier import LLMQuantifier
from pros_cons_processor import ProsConsProcessor
from bias_analyzer import BiasAnalyzer, BiasAnalysisResult
from visualizer import Visualizer
from utils import logger


class ReviewBiasAnalysisPipeline:
    """å®¡ç¨¿åå·®åˆ†æPipeline"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        output_dir: Optional[Path] = None
    ):
        """
        åˆå§‹åŒ–Pipeline
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            output_dir: è¾“å‡ºç›®å½•
        """
        # éªŒè¯é…ç½®
        Config.validate()
        Config.display()
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.data_loader = DataLoader()
        self.feature_extractor = FeatureExtractor(api_key, base_url, model)
        self.quantifier = LLMQuantifier(api_key, base_url, model)
        self.processor = ProsConsProcessor()
        self.analyzer = BiasAnalyzer()
        self.visualizer = Visualizer(output_dir)
        
        # æ•°æ®å­˜å‚¨
        self.papers: List[Paper] = []
        self.analysis_results: List[BiasAnalysisResult] = []
        
        # ä¸­é—´æ–‡ä»¶è·¯å¾„
        self.extraction_file: Optional[Path] = None
        self.anonymized_file: Optional[Path] = None
        self.mapping_file: Optional[Path] = None
        self.quantified_file: Optional[Path] = None
        
        logger.info("=" * 70)
        logger.info("å®¡ç¨¿åå·®åˆ†æPipelineå·²åˆå§‹åŒ–")
        logger.info("=" * 70)
    
    # ========== æ•°æ®åŠ è½½ ==========
    
    def load_data(
        self,
        file_path: Union[str, Path],
        format: str = "json"
    ) -> 'ReviewBiasAnalysisPipeline':
        """
        åŠ è½½æ•°æ®
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            format: æ•°æ®æ ¼å¼ ('json', 'csv', 'openreview_json')
            
        Returns:
            selfï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰
        """
        logger.info(f"\n{'='*70}")
        logger.info("æ­¥éª¤ 0: æ•°æ®åŠ è½½")
        logger.info(f"{'='*70}")
        
        if format == "json":
            self.papers = self.data_loader.load_from_json(file_path)
        elif format == "csv":
            self.papers = self.data_loader.load_from_csv(file_path)
        elif format == "openreview_json":
            self.papers = self.data_loader.load_from_openreview_json(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {format}")
        
        self.data_loader.display_statistics()
        
        return self
    
    def load_from_openreview(
        self,
        directory: Union[str, Path]
    ) -> 'ReviewBiasAnalysisPipeline':
        """
        ä»OpenReviewç›®å½•åŠ è½½æ•°æ®
        
        Args:
            directory: OpenReviewæ•°æ®ç›®å½•
            
        Returns:
            selfï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰
        """
        return self.load_data(directory, format="openreview_json")
    
    def load_paper_pdfs(self) -> 'ReviewBiasAnalysisPipeline':
        """
        ä¸ºå½“å‰åˆ—è¡¨ä¸­çš„æ‰€æœ‰è®ºæ–‡åŠ è½½ PDF å†…å®¹ï¼ˆå¦‚æœå°šæœªåŠ è½½ï¼‰
        å®ç°é€‚é…æ€§ä¼˜åŒ–ï¼šä»…å¤„ç†éœ€è¦åˆ†æçš„è®ºæ–‡
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"PDFæå–: æ­£åœ¨ä¸º {len(self.papers)} ç¯‡å¾…åˆ†æè®ºæ–‡æå– PDF æ–‡æœ¬")
        logger.info(f"{'='*70}")
        
        loaded_count = 0
        for paper in self.papers:
            if not paper.paper_content and paper.source_dir:
                self.data_loader._load_pdf_content(paper, paper.source_dir)
                if paper.paper_content:
                    loaded_count += 1
        
        logger.info(f"æˆåŠŸæå– {loaded_count} ç¯‡è®ºæ–‡çš„ PDF å†…å®¹")
        
        return self
    
    # ========== æ­¥éª¤1: ç‰¹å¾æå– ==========
    
    def step1_extract_features(self) -> 'ReviewBiasAnalysisPipeline':
        """
        æ­¥éª¤1: ä½¿ç”¨LLMæå–æ¯ä¸ªå®¡ç¨¿äººçš„ä¼˜ç¼ºç‚¹
        
        Returns:
            selfï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰
        """
        logger.info(f"\n{'='*70}")
        logger.info("æ­¥éª¤ 1: ç‰¹å¾æå–ï¼ˆç‹¬ç«‹æå–æ¯ä¸ªå®¡ç¨¿äººçš„ä¼˜ç¼ºç‚¹ï¼‰")
        logger.info(f"{'='*70}")
        
        if not self.papers:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")
        
        # æå–ä¼˜ç¼ºç‚¹
        self.papers = self.feature_extractor.extract_from_papers(self.papers)
        
        # ä¿å­˜æå–ç»“æœåˆ°æ–‡ä»¶
        self.extraction_file = self.feature_extractor.save_extraction_results(self.papers)
        
        # æ˜¾ç¤ºæ‘˜è¦
        self.feature_extractor.display_extraction_summary(self.papers)
        
        return self
    
    # ========== æ­¥éª¤2: åŒ¿ååŒ–å¤„ç† ==========
    
    def step2_anonymize_and_shuffle(self) -> 'ReviewBiasAnalysisPipeline':
        """
        æ­¥éª¤2: å¤„ç†æå–ç»“æœï¼Œå»é™¤å®¡ç¨¿äººä¿¡æ¯ï¼Œæ‰“ä¹±é¡ºåº
        
        Returns:
            selfï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰
        """
        logger.info(f"\n{'='*70}")
        logger.info("æ­¥éª¤ 2: åŒ¿ååŒ–å¤„ç†ï¼ˆå»é™¤å®¡ç¨¿äººä¿¡æ¯ï¼Œæ‰“ä¹±é¡ºåºï¼‰")
        logger.info(f"{'='*70}")
        
        if not self.extraction_file:
            raise ValueError("è¯·å…ˆæ‰§è¡Œæ­¥éª¤1ï¼ˆç‰¹å¾æå–ï¼‰")
        
        # å¤„ç†å¹¶è¾“å‡ºåŒ¿ååŒ–æ–‡ä»¶
        self.anonymized_file, mapping = self.processor.process_extraction_file(
            self.extraction_file
        )
        
        # ä¿å­˜æ˜ å°„æ–‡ä»¶è·¯å¾„
        self.mapping_file = Config.ANONYMIZED_DIR / "original_mapping.json"
        
        logger.info(f"âœ“ åŒ¿ååŒ–å®Œæˆï¼Œä¼˜ç¼ºç‚¹é¡ºåºå·²éšæœºæ‰“ä¹±")
        
        return self
    
    # ========== æ­¥éª¤3: æƒé‡é‡åŒ– ==========
    
    def step3_quantify_weights(self) -> 'ReviewBiasAnalysisPipeline':
        """
        æ­¥éª¤3: åŸºäºåŒ¿ååŒ–æ–‡ä»¶å’ŒPDFå†…å®¹ï¼Œä½¿ç”¨LLMé‡åŒ–æƒé‡
        
        Returns:
            selfï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰
        """
        logger.info(f"\n{'='*70}")
        logger.info("æ­¥éª¤ 3: æƒé‡é‡åŒ–ï¼ˆåŸºäºåŒ¿åä¼˜ç¼ºç‚¹+è®ºæ–‡å…¨æ–‡ï¼‰")
        logger.info(f"{'='*70}")
        
        if not self.anonymized_file:
            raise ValueError("è¯·å…ˆæ‰§è¡Œæ­¥éª¤2ï¼ˆåŒ¿ååŒ–å¤„ç†ï¼‰")
        
        # é‡åŒ–å¹¶è¾“å‡ºç»“æœ
        self.quantified_file = self.quantifier.quantify_anonymized_file(
            self.anonymized_file,
            self.papers
        )
        
        return self
    
    # ========== æ­¥éª¤4: åŒ¹é…å¹¶è®¡ç®—åˆ†æ•° ==========
    
    def step4_match_and_calculate(self) -> 'ReviewBiasAnalysisPipeline':
        """
        æ­¥éª¤4: ä»£ç é€»è¾‘åŒ¹é…å›å®¡ç¨¿äººï¼Œçº¿æ€§ç›¸åŠ å¾—åˆ†æ•°
        
        æ³¨æ„: æ­¤æ­¥éª¤ä¸ä½¿ç”¨LLMï¼Œå®Œå…¨ä½¿ç”¨ä»£ç é€»è¾‘
        
        Returns:
            selfï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰
        """
        logger.info(f"\n{'='*70}")
        logger.info("æ­¥éª¤ 4: åŒ¹é…è®¡ç®—ï¼ˆä»£ç é€»è¾‘åŒ¹é…å®¡ç¨¿äººï¼Œçº¿æ€§ç›¸åŠ ï¼‰")
        logger.info(f"{'='*70}")
        
        if not self.quantified_file or not self.mapping_file:
            raise ValueError("è¯·å…ˆæ‰§è¡Œæ­¥éª¤3ï¼ˆæƒé‡é‡åŒ–ï¼‰")
        
        # åŒ¹é…å¹¶è®¡ç®—åˆ†æ•°
        self.papers = self.processor.match_and_calculate_scores(
            self.quantified_file,
            self.mapping_file,
            self.papers
        )
        
        # æ˜¾ç¤ºé‡åŒ–æ‘˜è¦
        self.quantifier.display_quantification_summary(self.papers)
        
        return self
    
    # ========== æ‰¹æ¬¡å¤„ç† ==========
    
    def _split_into_batches(self, papers: List[Paper], batch_size: int) -> List[List[Paper]]:
        """å°†è®ºæ–‡åˆ—è¡¨åˆ†å‰²æˆæ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(papers), batch_size):
            batches.append(papers[i:i + batch_size])
        return batches
    
    def _process_batch(
        self, 
        batch_papers: List[Paper], 
        batch_index: int, 
        total_batches: int
    ) -> tuple:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„è®ºæ–‡ï¼ˆæ­¥éª¤1-3ï¼‰
        
        Args:
            batch_papers: å½“å‰æ‰¹æ¬¡çš„è®ºæ–‡åˆ—è¡¨
            batch_index: æ‰¹æ¬¡ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
            total_batches: æ€»æ‰¹æ¬¡æ•°
            
        Returns:
            (extraction_data, anonymized_data, quantified_data, mapping_data)
        """
        batch_num = batch_index + 1
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (å…± {len(batch_papers)} ç¯‡è®ºæ–‡)")
        logger.info(f"{'='*70}")
        
        # ä¸´æ—¶å­˜å‚¨å½“å‰æ‰¹æ¬¡çš„è®ºæ–‡
        original_papers = self.papers
        self.papers = batch_papers
        
        # 0. æå–PDFå†…å®¹
        self.load_paper_pdfs()
        
        # 1. ç‰¹å¾æå–
        logger.info(f"\n[æ‰¹æ¬¡{batch_num}] æ­¥éª¤1: ç‰¹å¾æå–")
        self.papers = self.feature_extractor.extract_from_papers(self.papers)
        
        # æ”¶é›†æå–æ•°æ®
        extraction_data = []
        for paper in self.papers:
            paper_data = {
                "paper_id": paper.paper_id,
                "title": paper.title,
                "abstract": paper.abstract,
                "reviews": []
            }
            for review in paper.reviews:
                review_data = {
                    "reviewer_id": review.reviewer_id,
                    "actual_score": review.actual_score,
                    "pros": review.pros,
                    "cons": review.cons
                }
                paper_data["reviews"].append(review_data)
            extraction_data.append(paper_data)
        
        # 2. åŒ¿ååŒ–å¤„ç†ï¼ˆç›´æ¥åœ¨å†…å­˜ä¸­å¤„ç†ï¼‰
        logger.info(f"\n[æ‰¹æ¬¡{batch_num}] æ­¥éª¤2: åŒ¿ååŒ–å¤„ç†")
        anonymized_data, mapping_data = self.processor.anonymize_in_memory(extraction_data)
        
        # æ­¥éª¤ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIè¯·æ±‚è¿‡äºå¯†é›†
        logger.info(f"  â³ æ­¥éª¤é—´éš”ç­‰å¾… {Config.BATCH_DELAY:.1f} ç§’...")
        time.sleep(Config.BATCH_DELAY)
        
        # 3. æƒé‡é‡åŒ–
        logger.info(f"\n[æ‰¹æ¬¡{batch_num}] æ­¥éª¤3: æƒé‡é‡åŒ–")
        quantified_data = self._quantify_batch_in_memory(anonymized_data, self.papers)
        
        # æ¢å¤åŸå§‹è®ºæ–‡åˆ—è¡¨
        self.papers = original_papers
        
        logger.info(f"\nâœ“ æ‰¹æ¬¡ {batch_num}/{total_batches} å¤„ç†å®Œæˆ")
        
        return extraction_data, anonymized_data, quantified_data, mapping_data
    
    def _quantify_batch_in_memory(
        self, 
        anonymized_data: List[dict], 
        papers: List[Paper]
    ) -> List[dict]:
        """åœ¨å†…å­˜ä¸­é‡åŒ–å•ä¸ªæ‰¹æ¬¡ï¼Œæ”¯æŒæ™ºèƒ½å»¶è¿Ÿ"""
        from utils import safe_json_parse, ProgressTracker
        
        paper_content_map = {p.paper_id: p.paper_content for p in papers}
        quantified_results = []
        consecutive_failures = 0
        max_consecutive_failures = 3
        
        for idx, paper_data in enumerate(anonymized_data):
            paper_id = paper_data["paper_id"]
            title = paper_data["title"]
            abstract = paper_data["abstract"]
            pros = paper_data["pros"]
            cons = paper_data["cons"]
            
            paper_content = paper_content_map.get(paper_id, "")
            
            logger.info(f"  [{idx+1}/{len(anonymized_data)}] æ­£åœ¨é‡åŒ–: {title[:45]}...")
            
            pros_count = len(pros)
            cons_count = len(cons)
            
            pros_text = "\n".join([
                f"{i+1}. [{p.get('category', 'æœªåˆ†ç±»')}] {p.get('description', '')}"
                for i, p in enumerate(pros)
            ]) if pros else "(æ— )"
            
            cons_text = "\n".join([
                f"{i+1}. [{c.get('category', 'æœªåˆ†ç±»')}] {c.get('description', '')}"
                for i, c in enumerate(cons)
            ]) if cons else "(æ— )"
            
            from config import PromptTemplates
            prompt = PromptTemplates.QUANTIFY_WEIGHTS.format(
                title=title,
                abstract=abstract,
                paper_content=paper_content[:15000] if paper_content else "(æœªæä¾›è®ºæ–‡å…¨æ–‡)",
                pros_text=pros_text,
                cons_text=cons_text,
                pros_count=pros_count,
                cons_count=cons_count,
                min_score=Config.MIN_SCORE,
                max_score=Config.MAX_SCORE,
                base_score=Config.BASE_SCORE
            )
            
            try:
                response = self.quantifier._call_llm(prompt)
                result = safe_json_parse(response, default={
                    "pros_weights": [],
                    "cons_weights": [],
                    "expected_score_breakdown": {}
                })
                
                pros_weights = result.get("pros_weights", [])
                cons_weights = result.get("cons_weights", [])
                
                # è¡¥é½ç¼ºå¤±çš„æƒé‡
                while len(pros_weights) < pros_count:
                    i = len(pros_weights)
                    pros_weights.append({
                        "description": pros[i].get("description", "") if i < len(pros) else "",
                        "category": pros[i].get("category", "") if i < len(pros) else "",
                        "weight": 0.5,
                        "reasoning": "LLMæœªè¿”å›ï¼Œä½¿ç”¨é»˜è®¤å€¼"
                    })
                
                while len(cons_weights) < cons_count:
                    i = len(cons_weights)
                    cons_weights.append({
                        "description": cons[i].get("description", "") if i < len(cons) else "",
                        "category": cons[i].get("category", "") if i < len(cons) else "",
                        "weight": -0.5,
                        "reasoning": "LLMæœªè¿”å›ï¼Œä½¿ç”¨é»˜è®¤å€¼"
                    })
                
                quantified_results.append({
                    "paper_id": paper_id,
                    "title": title,
                    "pros_weights": pros_weights,
                    "cons_weights": cons_weights,
                    "expected_score_breakdown": result.get("expected_score_breakdown", {})
                })
                
                logger.info(f"    âœ“ å®Œæˆ: {len(pros_weights)} ä¼˜ç‚¹, {len(cons_weights)} ç¼ºç‚¹")
                consecutive_failures = 0
                
                # æ™ºèƒ½å»¶è¿Ÿ
                if idx < len(anonymized_data) - 1:
                    if (idx + 1) % 10 == 0:
                        # æ¯10ç¯‡é•¿ä¼‘æ¯
                        logger.info(f"  â³ é•¿ä¼‘æ¯ {Config.BATCH_DELAY:.0f} ç§’...")
                        time.sleep(Config.BATCH_DELAY)
                    else:
                        time.sleep(Config.REQUEST_DELAY)
                
            except Exception as e:
                consecutive_failures += 1
                logger.error(f"  âœ— é‡åŒ–å¤±è´¥: {e}")
                
                quantified_results.append({
                    "paper_id": paper_id,
                    "title": title,
                    "pros_weights": [
                        {"description": p.get("description", ""), "category": p.get("category", ""), "weight": 0, "reasoning": "é‡åŒ–å¤±è´¥"}
                        for p in pros
                    ],
                    "cons_weights": [
                        {"description": c.get("description", ""), "category": c.get("category", ""), "weight": 0, "reasoning": "é‡åŒ–å¤±è´¥"}
                        for c in cons
                    ],
                    "expected_score_breakdown": {}
                })
                
                # è¿ç»­å¤±è´¥æ—¶å¢åŠ ç­‰å¾…
                if consecutive_failures >= max_consecutive_failures:
                    wait_time = Config.BATCH_DELAY * 3
                    logger.warning(f"âš ï¸ è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œç­‰å¾… {wait_time:.0f} ç§’...")
                    time.sleep(wait_time)
                    consecutive_failures = 0
        
        return quantified_results
    
    def _merge_batch_results(
        self,
        all_extraction: List[List[dict]],
        all_anonymized: List[List[dict]],
        all_quantified: List[List[dict]],
        all_mapping: List[dict]
    ) -> tuple:
        """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ"""
        merged_extraction = []
        merged_anonymized = []
        merged_quantified = []
        merged_mapping = {}
        
        for batch_ext in all_extraction:
            merged_extraction.extend(batch_ext)
        
        for batch_anon in all_anonymized:
            merged_anonymized.extend(batch_anon)
        
        for batch_quant in all_quantified:
            merged_quantified.extend(batch_quant)
        
        for batch_map in all_mapping:
            merged_mapping.update(batch_map)
        
        return merged_extraction, merged_anonymized, merged_quantified, merged_mapping
    
    # ========== å®Œæ•´æµç¨‹ ==========
    
    def run_full_analysis(self, batch_size: int = None) -> dict:
        """
        è¿è¡Œå®Œæ•´çš„å››æ­¥éª¤åˆ†ææµç¨‹ï¼ˆæ”¯æŒæ‰¹æ¬¡å¤„ç†ï¼‰
        
        æµç¨‹:
        1. åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹10ç¯‡ï¼‰:
           - ç‰¹å¾æå– -> è¾“å‡ºæ–‡ä»¶
           - åŒ¿ååŒ–å¤„ç† -> è¾“å‡ºæ–°æ–‡ä»¶
           - æƒé‡é‡åŒ– -> è¾“å‡ºé‡åŒ–æ–‡ä»¶
        2. åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        3. åŒ¹é…è®¡ç®— -> æ›´æ–°è®ºæ–‡æ•°æ®
        4. åå·®åˆ†æ
        5. å¯è§†åŒ–
        
        Args:
            batch_size: æ¯æ‰¹å¤„ç†çš„è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤ä¸º Config.BATCH_SIZE
        
        Returns:
            åˆ†æç»“æœæ‘˜è¦å­—å…¸
        """
        batch_size = batch_size or Config.BATCH_SIZE
        
        logger.info("\n" + "="*70)
        logger.info(f"å¼€å§‹å®Œæ•´åˆ†ææµç¨‹ï¼ˆæ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} ç¯‡ï¼‰")
        logger.info("="*70)
        
        # åˆ†å‰²æˆæ‰¹æ¬¡
        batches = self._split_into_batches(self.papers, batch_size)
        total_batches = len(batches)
        
        logger.info(f"å…± {len(self.papers)} ç¯‡è®ºæ–‡ï¼Œåˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†")
        
        # å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        all_extraction = []
        all_anonymized = []
        all_quantified = []
        all_mapping = []
        
        # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        for i, batch_papers in enumerate(batches):
            extraction_data, anonymized_data, quantified_data, mapping_data = \
                self._process_batch(batch_papers, i, total_batches)
            
            all_extraction.append(extraction_data)
            all_anonymized.append(anonymized_data)
            all_quantified.append(quantified_data)
            all_mapping.append(mapping_data)
            
            # æ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿ
            if i < total_batches - 1:
                logger.info(f"\nâ³ ç­‰å¾… {Config.BATCH_DELAY} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹æ¬¡...")
                time.sleep(Config.BATCH_DELAY)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        logger.info(f"\n{'='*70}")
        logger.info("åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ")
        logger.info(f"{'='*70}")
        
        merged_extraction, merged_anonymized, merged_quantified, merged_mapping = \
            self._merge_batch_results(all_extraction, all_anonymized, all_quantified, all_mapping)
        
        # ä¿å­˜åˆå¹¶åçš„ç»“æœåˆ°æ–‡ä»¶
        self.extraction_file = Config.EXTRACTION_DIR / "extraction_results.json"
        with open(self.extraction_file, 'w', encoding='utf-8') as f:
            json.dump(merged_extraction, f, ensure_ascii=False, indent=2)
        logger.info(f"  å·²ä¿å­˜æå–ç»“æœ: {self.extraction_file}")
        
        self.anonymized_file = Config.ANONYMIZED_DIR / "anonymized_pros_cons.json"
        with open(self.anonymized_file, 'w', encoding='utf-8') as f:
            json.dump(merged_anonymized, f, ensure_ascii=False, indent=2)
        logger.info(f"  å·²ä¿å­˜åŒ¿ååŒ–ç»“æœ: {self.anonymized_file}")
        
        self.mapping_file = Config.ANONYMIZED_DIR / "original_mapping.json"
        with open(self.mapping_file, 'w', encoding='utf-8') as f:
            json.dump(merged_mapping, f, ensure_ascii=False, indent=2)
        logger.info(f"  å·²ä¿å­˜æ˜ å°„æ–‡ä»¶: {self.mapping_file}")
        
        self.quantified_file = Config.QUANTIFIED_DIR / "quantified_weights.json"
        with open(self.quantified_file, 'w', encoding='utf-8') as f:
            json.dump(merged_quantified, f, ensure_ascii=False, indent=2)
        logger.info(f"  å·²ä¿å­˜é‡åŒ–ç»“æœ: {self.quantified_file}")
        
        # 4. åŒ¹é…è®¡ç®—ï¼ˆä½¿ç”¨åˆå¹¶åçš„æ–‡ä»¶ï¼‰
        self.step4_match_and_calculate()
        
        # 5. æ˜¾ç¤ºæ‘˜è¦
        self.feature_extractor.display_extraction_summary(self.papers)
        self.quantifier.display_quantification_summary(self.papers)
        
        # 6. ä¿å­˜æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†æŠ¥å‘Š
        self.save_individual_reports()
        
        # 7. åå·®åˆ†æ
        self.analyze_bias()
        
        # 8. ç”Ÿæˆå¯è§†åŒ–
        self.generate_visualizations()
        
        # 9. ç”Ÿæˆæ‘˜è¦
        summary = {
            "data_statistics": self.data_loader.get_statistics(),
            "extraction_summary": self.feature_extractor.get_extraction_summary(self.papers),
            "quantification_summary": self.quantifier.get_quantification_summary(self.papers),
            "bias_statistics": self.analyzer.global_statistics(self.analysis_results),
            "batch_info": {
                "batch_size": batch_size,
                "total_batches": total_batches,
                "total_papers": len(self.papers)
            },
            "output_files": {
                "extraction": str(self.extraction_file),
                "anonymized": str(self.anonymized_file),
                "mapping": str(self.mapping_file),
                "quantified": str(self.quantified_file)
            }
        }
        
        logger.info("\n" + "="*70)
        logger.info("å®Œæ•´åˆ†ææµç¨‹å·²å®Œæˆï¼")
        logger.info("="*70)
        logger.info(f"\næ‰¹æ¬¡å¤„ç†ä¿¡æ¯:")
        logger.info(f"  æ¯æ‰¹å¤§å°: {batch_size} ç¯‡")
        logger.info(f"  æ€»æ‰¹æ¬¡æ•°: {total_batches}")
        logger.info(f"  æ€»è®ºæ–‡æ•°: {len(self.papers)}")
        logger.info(f"\nä¸­é—´æ–‡ä»¶:")
        logger.info(f"  æ­¥éª¤1 æå–ç»“æœ: {self.extraction_file}")
        logger.info(f"  æ­¥éª¤2 åŒ¿ååŒ–æ–‡ä»¶: {self.anonymized_file}")
        logger.info(f"  æ­¥éª¤2 æ˜ å°„æ–‡ä»¶: {self.mapping_file}")
        logger.info(f"  æ­¥éª¤3 é‡åŒ–ç»“æœ: {self.quantified_file}")
        
        return summary
    
    # ========== åå·®åˆ†æ ==========
    
    def analyze_bias(self) -> List[BiasAnalysisResult]:
        """
        åˆ†æåå·®
        
        Returns:
            åå·®åˆ†æç»“æœåˆ—è¡¨
        """
        logger.info(f"\n{'='*70}")
        logger.info("æ­¥éª¤ 5: åå·®åˆ†æ")
        logger.info(f"{'='*70}")
        
        if not self.papers:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")
        
        # æ£€æŸ¥æ˜¯å¦å·²è®¡ç®—æœŸæœ›åˆ†æ•°
        if self.papers[0].reviews[0].expected_score is None:
            logger.warning("æœªæ£€æµ‹åˆ°æœŸæœ›åˆ†æ•°ï¼Œè¯·å…ˆæ‰§è¡Œæ­¥éª¤1-4")
            return []
        
        self.analysis_results = self.analyzer.analyze_papers(self.papers)
        self.analyzer.display_summary(self.analysis_results)
        
        return self.analysis_results
    
    # ========== å¯è§†åŒ– ==========
    
    def generate_visualizations(self) -> 'ReviewBiasAnalysisPipeline':
        """
        ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        
        Returns:
            selfï¼ˆæ”¯æŒé“¾å¼è°ƒç”¨ï¼‰
        """
        logger.info(f"\n{'='*70}")
        logger.info("æ­¥éª¤ 6: ç”Ÿæˆå¯è§†åŒ–")
        logger.info(f"{'='*70}")
        
        if not self.analysis_results:
            logger.warning("æœªæ£€æµ‹åˆ°åˆ†æç»“æœï¼Œè·³è¿‡å¯è§†åŒ–")
            return self
        
        self.visualizer.generate_all_plots(self.papers, self.analysis_results)
        
        return self
    
    # ========== ç»“æœä¿å­˜ ==========
    
    def save_individual_reports(self, output_dir: Optional[Union[str, Path]] = None):
        """
        ä¸ºæ¯ç¯‡è®ºæ–‡ä¿å­˜è¯¦ç»†çš„åˆ†ææŠ¥å‘Š
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º Config.DETAILS_DIR
        """
        logger.info(f"\n{'='*70}")
        logger.info("ä¿å­˜æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†åˆ†ææŠ¥å‘Š")
        logger.info(f"{'='*70}")
        
        if not self.papers:
            logger.warning("æ²¡æœ‰è®ºæ–‡æ•°æ®ï¼Œè·³è¿‡è¯¦ç»†æŠ¥å‘Šä¿å­˜")
            return
            
        details_dir = Path(output_dir) if output_dir else Config.DETAILS_DIR
        details_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"æ­£åœ¨ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°: {details_dir}")
        
        for paper in self.papers:
            file_name = f"{paper.paper_id.replace('/', '_')}_details.md"
            file_path = details_dir / file_name
            
            content = self._generate_paper_report_content(paper)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
        logger.info(f"å·²æˆåŠŸä¿å­˜ {len(self.papers)} ç¯‡è®ºæ–‡çš„è¯¦ç»†æŠ¥å‘Š")

    def _generate_paper_report_content(self, paper: Paper) -> str:
        """ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„æŠ¥å‘Šå†…å®¹"""
        content = [
            f"# è®ºæ–‡è¯¦ç»†åˆ†ææŠ¥å‘Š: {paper.title}",
            f"\n**è®ºæ–‡ID:** {paper.paper_id}",
            f"\n## æ‘˜è¦\n{paper.abstract}",
        ]
        
        # æ·»åŠ è®ºæ–‡å…¨æ–‡ä¿¡æ¯
        if paper.paper_content:
            content.append(f"\n## è®ºæ–‡å…¨æ–‡å†…å®¹ (æˆªå–å‰500å­—)\n{paper.paper_content[:500]}...")
        else:
            content.append("\n## è®ºæ–‡å…¨æ–‡å†…å®¹\n(æœªæå–åˆ°è®ºæ–‡å…¨æ–‡)")
        
        content.append(f"\n## å„å®¡ç¨¿äººè¯¦ç»†åˆ†æ\n")
        
        for i, review in enumerate(paper.reviews):
            content.append(f"### å®¡ç¨¿äºº {review.reviewer_id}")
            content.append(f"- **å®é™…åˆ†æ•°:** {review.actual_score}")
            content.append(f"- **æœŸæœ›åˆ†æ•°:** {review.expected_score:.2f}" if review.expected_score is not None else "- **æœŸæœ›åˆ†æ•°:** æœªè®¡ç®—")
            content.append(f"- **åå·® (å®é™… - æœŸæœ›):** {review.bias:+.2f}" if review.bias is not None else "- **åå·®:** æœªè®¡ç®—")
            
            # ä¼˜ç‚¹
            content.append("\n#### ä¼˜ç‚¹ (Pros)")
            if review.pros_weights:
                for pw in review.pros_weights:
                    content.append(f"- **[{pw.get('category', 'æœªåˆ†ç±»')}]** {pw.get('description', '')}")
                    content.append(f"  - æƒé‡: `{pw.get('weight', 0):+.2f}`")
                    content.append(f"  - ç†ç”±: {pw.get('reasoning', 'æ— ')}")
            elif review.pros:
                for p in review.pros:
                    content.append(f"- **[{p.get('category', 'æœªåˆ†ç±»')}]** {p.get('description', '')}")
            else:
                content.append("- (æ— )")
            
            # ç¼ºç‚¹
            content.append("\n#### ç¼ºç‚¹ (Cons)")
            if review.cons_weights:
                for cw in review.cons_weights:
                    content.append(f"- **[{cw.get('category', 'æœªåˆ†ç±»')}]** {cw.get('description', '')}")
                    content.append(f"  - æƒé‡: `{cw.get('weight', 0):+.2f}`")
                    content.append(f"  - ç†ç”±: {cw.get('reasoning', 'æ— ')}")
            elif review.cons:
                for c in review.cons:
                    content.append(f"- **[{c.get('category', 'æœªåˆ†ç±»')}]** {c.get('description', '')}")
            else:
                content.append("- (æ— )")
            
            content.append("\n" + "-"*30 + "\n")
        
        return "\n".join(content)
    
    def save_results(self, output_file: Optional[Union[str, Path]] = None):
        """
        ä¿å­˜åˆ†æç»“æœ
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            output_file = Config.OUTPUT_DIR / "analysis_results.json"
        else:
            output_file = Path(output_file)
        
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡è¾“å‡ºæ•°æ®
        output_data = {
            "papers": [paper.to_dict() for paper in self.papers],
            "analysis_results": [result.to_dict() for result in self.analysis_results],
            "global_statistics": self.analyzer.global_statistics(self.analysis_results) if self.analysis_results else {},
        }
        
        # ä¿å­˜ä¸ºJSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def save_papers(self, output_file: Optional[Union[str, Path]] = None):
        """
        ä¿å­˜å¤„ç†åçš„è®ºæ–‡æ•°æ®
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            output_file = Config.OUTPUT_DIR / "processed_papers.json"
        else:
            output_file = Path(output_file)
        
        self.data_loader.save_to_json(output_file)
    
    def generate_report(self, output_file: Optional[Union[str, Path]] = None):
        """
        ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            output_file = Config.OUTPUT_DIR / "analysis_report.txt"
        else:
            output_file = Path(output_file)
        
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        if not self.analysis_results:
            logger.warning("æ²¡æœ‰åˆ†æç»“æœï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return
        
        report = self.analyzer.generate_summary_report(self.analysis_results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    # ========== é«˜çº§åŠŸèƒ½ ==========
    
    def identify_problematic_papers(self, threshold: float = 2.0) -> List[dict]:
        """
        è¯†åˆ«é—®é¢˜è®ºæ–‡ï¼ˆé«˜åå·®ï¼‰
        
        Args:
            threshold: åå·®é˜ˆå€¼
            
        Returns:
            é—®é¢˜è®ºæ–‡åˆ—è¡¨
        """
        if not self.analysis_results:
            logger.warning("è¯·å…ˆè¿è¡Œåå·®åˆ†æ")
            return []
        
        problematic = self.analyzer.identify_high_bias_cases(
            self.analysis_results, 
            threshold=threshold
        )
        
        logger.info(f"è¯†åˆ«å‡º {len(problematic)} ä¸ªé«˜åå·®æ¡ˆä¾‹")
        
        return problematic
    
    def get_summary(self) -> dict:
        """
        è·å–å®Œæ•´æ‘˜è¦
        
        Returns:
            æ‘˜è¦å­—å…¸
        """
        if not self.analysis_results:
            logger.warning("è¯·å…ˆè¿è¡Œåˆ†æ")
            return {}
        
        return {
            "data": self.data_loader.get_statistics(),
            "extraction": self.feature_extractor.get_extraction_summary(self.papers),
            "quantification": self.quantifier.get_quantification_summary(self.papers),
            "bias_analysis": self.analyzer.global_statistics(self.analysis_results),
        }


if __name__ == "__main__":
    # æµ‹è¯•Pipeline
    import numpy as np
    from data_loader import Paper, Review
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_papers = []
    for i in range(2):
        paper = Paper(
            paper_id=f"paper_{i}",
            title=f"Test Paper {i}: An Innovative Approach",
            abstract="This paper presents a novel method for solving complex problems.",
            paper_content="Full paper content here..."
        )
        
        for j in range(2):
            review = Review(
                reviewer_id=f"reviewer_{j}",
                review_text=f"""
                This paper has several strengths and weaknesses.
                
                Strengths:
                - Novel approach to the problem
                - Good experimental design
                
                Weaknesses:
                - Limited baseline comparisons
                - Writing could be improved
                """,
                actual_score=np.random.uniform(5, 9)
            )
            paper.add_review(review)
        
        test_papers.append(paper)
    
    # ä¿å­˜æµ‹è¯•æ•°æ®
    test_data_file = Path("test_reviews.json")
    data_to_save = [p.to_dict() for p in test_papers]
    with open(test_data_file, 'w', encoding='utf-8') as f:
        json.dump(data_to_save, f, ensure_ascii=False, indent=2)
    
    print("\nâœ“ æµ‹è¯•æ•°æ®å·²åˆ›å»º")
    print(f"æ•°æ®æ–‡ä»¶: {test_data_file}")
    print("\nå¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç è¿›è¡Œæµ‹è¯•:")
    print("""
# åˆå§‹åŒ–pipeline
pipeline = ReviewBiasAnalysisPipeline()

# åŠ è½½æ•°æ®
pipeline.load_data("test_reviews.json")

# è¿è¡Œå®Œæ•´åˆ†æï¼ˆéœ€è¦é…ç½®APIå¯†é’¥ï¼‰
# results = pipeline.run_full_analysis()

# æˆ–åˆ†æ­¥æ‰§è¡Œ
# pipeline.step1_extract_features()
# pipeline.step2_anonymize_and_shuffle()
# pipeline.step3_quantify_weights()
# pipeline.step4_match_and_calculate()
# pipeline.analyze_bias()
# pipeline.generate_visualizations()

# ä¿å­˜ç»“æœ
# pipeline.save_results()
# pipeline.generate_report()
    """)
    
    # æ¸…ç†
    test_data_file.unlink()
