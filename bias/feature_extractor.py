"""
ç‰¹å¾æå–æ¨¡å—
ä½¿ç”¨LLMä»å®¡ç¨¿æ„è§ä¸­æå–ç»“æ„åŒ–çš„ä¼˜ç¼ºç‚¹
æ­¥éª¤1: ç‹¬ç«‹æå–æ¯ä¸ªå®¡ç¨¿äººçš„ä¼˜ç¼ºç‚¹å¹¶ä¿å­˜åˆ°æ–‡ä»¶
"""

import json
from typing import List, Dict, Any
from pathlib import Path
from openai import OpenAI

import time
from config import Config, PromptTemplates
from data_loader import Paper, Review
from utils import logger, cached, retry_on_failure, safe_json_parse, ProgressTracker


class FeatureExtractor:
    """ç‰¹å¾æå–å™¨ - ä»å®¡ç¨¿æ–‡æœ¬ä¸­æå–ä¼˜ç¼ºç‚¹"""
    
    def __init__(self, api_key: str = None, base_url: str = None, model: str = None):
        """
        åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.api_key = api_key or Config.OPENAI_API_KEY
        self.base_url = base_url or Config.OPENAI_BASE_URL
        self.model = model or Config.MODEL_NAME
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        client_kwargs = {"api_key": self.api_key}
        if self.base_url:
            client_kwargs["base_url"] = self.base_url
        
        self.client = OpenAI(**client_kwargs)
        
        logger.info(f"ç‰¹å¾æå–å™¨å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡å‹: {self.model}")
    
    @retry_on_failure()
    @cached
    def _call_llm(self, prompt: str) -> str:
        """
        è°ƒç”¨LLM API
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è®ºæ–‡å®¡ç¨¿ä¸“å®¶ï¼Œæ“…é•¿åˆ†æå®¡ç¨¿æ„è§ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=Config.TEMPERATURE,
                max_tokens=Config.MAX_TOKENS,
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def extract_pros_cons_from_paper(
        self, 
        paper: Paper
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        ä½¿ç”¨LLMä¸€æ¬¡æ€§æå–ä¸€ç¯‡è®ºæ–‡æ‰€æœ‰å®¡ç¨¿æ„è§çš„ä¼˜ç¼ºç‚¹
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡ï¼ˆåŒ…å«æ‰€æœ‰å®¡ç¨¿æ„è§ï¼‰
            
        Returns:
            åŒ…å«æ¯ä¸ªå®¡ç¨¿äººä¼˜ç¼ºç‚¹çš„å­—å…¸
        """
        logger.info(f"æ­£åœ¨æå–è®ºæ–‡ {paper.title[:50]}... çš„æ‰€æœ‰å®¡ç¨¿ä¼˜ç¼ºç‚¹")
        
        # æ„å»ºæ‰€æœ‰å®¡ç¨¿æ„è§çš„æ–‡æœ¬
        all_reviews_text = ""
        for i, review in enumerate(paper.reviews):
            all_reviews_text += f"\n{'='*40}\n"
            all_reviews_text += f"ã€å®¡ç¨¿äºº {review.reviewer_id}ã€‘\n"
            all_reviews_text += f"{'='*40}\n"
            all_reviews_text += f"{review.review_text}\n"
        
        # æ„å»ºæç¤ºè¯
        categories_str = ", ".join(Config.CATEGORIES)
        prompt = PromptTemplates.EXTRACT_PROS_CONS_BATCH.format(
            title=paper.title,
            abstract=paper.abstract,
            num_reviewers=len(paper.reviews),
            all_reviews_text=all_reviews_text,
            categories=categories_str
        )
        
        # è°ƒç”¨LLMï¼ˆä¸€æ¬¡è°ƒç”¨å¤„ç†æ‰€æœ‰å®¡ç¨¿æ„è§ï¼‰
        try:
            response = self._call_llm(prompt)
            
            # è§£æJSONå“åº”
            result = safe_json_parse(response, default={"reviewers": []})
            
            # æ„å»º reviewer_id -> ä¼˜ç¼ºç‚¹ çš„æ˜ å°„
            reviewer_results = {}
            for reviewer_data in result.get("reviewers", []):
                reviewer_id = reviewer_data.get("reviewer_id", "")
                pros = reviewer_data.get("pros", [])
                cons = reviewer_data.get("cons", [])
                
                # ç¡®ä¿æ¯ä¸ªä¼˜ç¼ºç‚¹éƒ½æœ‰å¿…éœ€çš„å­—æ®µ
                for pro in pros:
                    if "category" not in pro:
                        pro["category"] = "æœªåˆ†ç±»"
                    if "description" not in pro:
                        pro["description"] = ""
                
                for con in cons:
                    if "category" not in con:
                        con["category"] = "æœªåˆ†ç±»"
                    if "description" not in con:
                        con["description"] = ""
                
                reviewer_results[reviewer_id] = {
                    "pros": pros,
                    "cons": cons
                }
            
            # ä¸ºæ¯ä¸ªå®¡ç¨¿äººæ›´æ–°ç»“æœ
            for review in paper.reviews:
                if review.reviewer_id in reviewer_results:
                    data = reviewer_results[review.reviewer_id]
                    review.pros = data["pros"]
                    review.cons = data["cons"]
                else:
                    # å¦‚æœLLMæ²¡æœ‰è¿”å›è¯¥å®¡ç¨¿äººçš„ç»“æœï¼Œè®¾ä¸ºç©º
                    logger.warning(f"  âš  æœªæ‰¾åˆ°å®¡ç¨¿äºº {review.reviewer_id} çš„æå–ç»“æœ")
                    review.pros = []
                    review.cons = []
                
                logger.info(
                    f"  å®¡ç¨¿äºº {review.reviewer_id}: "
                    f"{len(review.pros)} ä¼˜ç‚¹, {len(review.cons)} ç¼ºç‚¹"
                )
            
            return reviewer_results
            
        except Exception as e:
            logger.error(f"LLMæå–ä¼˜ç¼ºç‚¹å¤±è´¥: {e}")
            # ä¸ºæ‰€æœ‰å®¡ç¨¿äººè®¾ç½®ç©ºç»“æœ
            for review in paper.reviews:
                review.pros = []
                review.cons = []
            return {}
    
    def extract_from_paper(self, paper: Paper) -> Paper:
        """
        ä»è®ºæ–‡çš„æ‰€æœ‰å®¡ç¨¿æ„è§ä¸­æå–ä¼˜ç¼ºç‚¹ï¼ˆä¸€æ¬¡LLMè°ƒç”¨ï¼‰
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡ï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
            
        Returns:
            æ›´æ–°åçš„è®ºæ–‡å¯¹è±¡
        """
        # ä¸€æ¬¡æ€§æå–è¯¥è®ºæ–‡æ‰€æœ‰å®¡ç¨¿äººçš„ä¼˜ç¼ºç‚¹
        self.extract_pros_cons_from_paper(paper)
        return paper
    
    def extract_from_papers(self, papers: List[Paper], checkpoint_interval: int = 5) -> List[Paper]:
        """
        æ‰¹é‡æå–å¤šç¯‡è®ºæ–‡çš„ä¼˜ç¼ºç‚¹
        æ¯ç¯‡è®ºæ–‡åªéœ€è¦ä¸€æ¬¡LLMè°ƒç”¨ï¼ˆè€Œä¸æ˜¯æ¯ä¸ªå®¡ç¨¿äººä¸€æ¬¡ï¼‰
        æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²æœ‰æå–ç»“æœçš„è®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
            checkpoint_interval: æ¯éš”å¤šå°‘ç¯‡ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            
        Returns:
            æ›´æ–°åçš„è®ºæ–‡åˆ—è¡¨
        """
        total_reviews = sum(len(p.reviews) for p in papers)
        logger.info(f"å¼€å§‹æ‰¹é‡ç‰¹å¾æå–ï¼Œå…± {len(papers)} ç¯‡è®ºæ–‡ ({total_reviews} æ¡å®¡ç¨¿)")
        logger.info(f"ğŸ“Œ ä¼˜åŒ–: æ¯ç¯‡è®ºæ–‡1æ¬¡APIè°ƒç”¨ï¼Œå…±éœ€ {len(papers)} æ¬¡APIè°ƒç”¨")
        
        # æ£€æŸ¥å·²æœ‰æå–ç»“æœçš„è®ºæ–‡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        papers_to_process = []
        papers_already_done = 0
        for paper in papers:
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å®¡ç¨¿éƒ½å·²æœ‰æå–ç»“æœ
            has_results = all(
                len(r.pros) > 0 or len(r.cons) > 0 
                for r in paper.reviews
            )
            if has_results:
                papers_already_done += 1
            else:
                papers_to_process.append(paper)
        
        if papers_already_done > 0:
            logger.info(f"ğŸ“‹ æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {papers_already_done} ç¯‡å·²å¤„ç†è®ºæ–‡")
        
        if not papers_to_process:
            logger.info("æ‰€æœ‰è®ºæ–‡å·²å¤„ç†å®Œæˆï¼Œæ— éœ€APIè°ƒç”¨")
            return papers
        
        logger.info(f"éœ€è¦å¤„ç†: {len(papers_to_process)} ç¯‡è®ºæ–‡")
        
        tracker = ProgressTracker(
            total=len(papers_to_process),
            description="ç‰¹å¾æå–"
        )
        
        consecutive_failures = 0
        max_consecutive_failures = 3
        
        for i, paper in enumerate(papers_to_process):
            try:
                self.extract_from_paper(paper)
                tracker.update(1)
                consecutive_failures = 0  # é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                
                # æ¯éš”ä¸€å®šæ•°é‡ä¿å­˜æ£€æŸ¥ç‚¹
                if (i + 1) % checkpoint_interval == 0:
                    logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹: å·²å¤„ç† {i + 1}/{len(papers_to_process)} ç¯‡")
                
                # è®ºæ–‡ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIè¯·æ±‚è¿‡äºå¯†é›†
                if i < len(papers_to_process) - 1:
                    # æ ¹æ®å¤„ç†è¿›åº¦åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
                    if i > 0 and i % 10 == 0:
                        # æ¯å¤„ç†10ç¯‡ï¼Œå¢åŠ ä¸€æ¬¡é•¿ä¼‘æ¯
                        long_delay = Config.BATCH_DELAY
                        logger.info(f"  â³ é•¿ä¼‘æ¯ {long_delay:.1f} ç§’ (å·²å¤„ç† {i+1} ç¯‡)...")
                        time.sleep(long_delay)
                    else:
                        logger.info(f"  â³ ç­‰å¾… {Config.REQUEST_DELAY:.1f} ç§’...")
                        time.sleep(Config.REQUEST_DELAY)
                        
            except Exception as e:
                consecutive_failures += 1
                logger.error(f"å¤„ç†è®ºæ–‡ {paper.title[:30]}... å¤±è´¥: {e}")
                
                # å¦‚æœè¿ç»­å¤±è´¥å¤šæ¬¡ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´
                if consecutive_failures >= max_consecutive_failures:
                    wait_time = Config.BATCH_DELAY * 2
                    logger.warning(f"âš ï¸ è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œç­‰å¾… {wait_time} ç§’åç»§ç»­...")
                    time.sleep(wait_time)
                    consecutive_failures = 0
        
        tracker.finish()
        
        return papers
    
    def save_extraction_results(self, papers: List[Paper], output_dir: Path = None) -> Path:
        """
        æ­¥éª¤1: å°†æå–ç»“æœä¿å­˜åˆ°æ–‡ä»¶
        ä¿å­˜æ ¼å¼: æ¯ä¸ªå®¡ç¨¿äººçš„ä¼˜ç¼ºç‚¹ï¼ŒåŒ…å«å®¡ç¨¿äººIDä¿¡æ¯
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆå·²æå–ä¼˜ç¼ºç‚¹ï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º Config.EXTRACTION_DIR
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_dir = output_dir or Config.EXTRACTION_DIR
        output_dir.mkdir(parents=True, exist_ok=True)
        
        extraction_data = []
        
        for paper in papers:
            paper_data = {
                "paper_id": paper.paper_id,
                "title": paper.title,
                "abstract": paper.abstract,
                "reviews": []
            }
            
            for review in paper.reviews:
                review_data = {
                    "reviewer_id": review.reviewer_id,
                    "actual_score": review.actual_score,
                    "pros": review.pros,
                    "cons": review.cons
                }
                paper_data["reviews"].append(review_data)
            
            extraction_data.append(paper_data)
        
        output_file = output_dir / "extraction_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(extraction_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç‰¹å¾æå–ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def compare_reviews_similarity(
        self,
        review1: Review,
        review2: Review,
        paper_title: str
    ) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸¤ä¸ªå®¡ç¨¿æ„è§çš„ç›¸ä¼¼åº¦
        
        Args:
            review1: ç¬¬ä¸€ä¸ªå®¡ç¨¿è®°å½•
            review2: ç¬¬äºŒä¸ªå®¡ç¨¿è®°å½•
            paper_title: è®ºæ–‡æ ‡é¢˜
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æç»“æœ
        """
        # æ ¼å¼åŒ–ä¼˜ç¼ºç‚¹
        pros_1 = "\n".join([f"- {p['description']}" for p in review1.pros])
        cons_1 = "\n".join([f"- {c['description']}" for c in review1.cons])
        pros_2 = "\n".join([f"- {p['description']}" for p in review2.pros])
        cons_2 = "\n".join([f"- {c['description']}" for c in review2.cons])
        
        # æ„å»ºæç¤ºè¯
        prompt = PromptTemplates.COMPARE_REVIEWS.format(
            title=paper_title,
            pros_1=pros_1 or "(æ— )",
            cons_1=cons_1 or "(æ— )",
            pros_2=pros_2 or "(æ— )",
            cons_2=cons_2 or "(æ— )"
        )
        
        # è°ƒç”¨LLM
        response = self._call_llm(prompt)
        
        # è§£æå“åº”
        result = safe_json_parse(response, default={
            "pros_similarity": 0.0,
            "cons_similarity": 0.0,
            "overall_similarity": 0.0,
            "common_pros": [],
            "common_cons": [],
            "unique_to_reviewer1": [],
            "unique_to_reviewer2": []
        })
        
        return result
    
    def analyze_paper_review_similarity(self, paper: Paper) -> List[Dict[str, Any]]:
        """
        åˆ†æä¸€ç¯‡è®ºæ–‡çš„æ‰€æœ‰å®¡ç¨¿æ„è§ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æç»“æœåˆ—è¡¨
        """
        if len(paper.reviews) < 2:
            logger.warning(f"è®ºæ–‡ {paper.title} çš„å®¡ç¨¿æ•°å°‘äº2ï¼Œè·³è¿‡ç›¸ä¼¼åº¦åˆ†æ")
            return []
        
        logger.info(f"åˆ†æè®ºæ–‡ {paper.title} çš„å®¡ç¨¿ç›¸ä¼¼åº¦...")
        
        similarities = []
        reviews = paper.reviews
        
        # ä¸¤ä¸¤æ¯”è¾ƒ
        for i in range(len(reviews)):
            for j in range(i + 1, len(reviews)):
                try:
                    similarity = self.compare_reviews_similarity(
                        review1=reviews[i],
                        review2=reviews[j],
                        paper_title=paper.title
                    )
                    
                    similarity["reviewer1_id"] = reviews[i].reviewer_id
                    similarity["reviewer2_id"] = reviews[j].reviewer_id
                    similarity["score1"] = reviews[i].actual_score
                    similarity["score2"] = reviews[j].actual_score
                    similarity["score_diff"] = abs(
                        reviews[i].actual_score - reviews[j].actual_score
                    )
                    
                    similarities.append(similarity)
                    
                    logger.info(
                        f"  {reviews[i].reviewer_id} vs {reviews[j].reviewer_id}: "
                        f"ç›¸ä¼¼åº¦={similarity['overall_similarity']:.2f}, "
                        f"åˆ†æ•°å·®={similarity['score_diff']:.1f}"
                    )
                    
                except Exception as e:
                    logger.error(f"æ¯”è¾ƒå®¡ç¨¿æ„è§æ—¶å‡ºé”™: {e}")
        
        return similarities
    
    def get_extraction_summary(self, papers: List[Paper]) -> Dict[str, Any]:
        """
        è·å–ç‰¹å¾æå–çš„æ‘˜è¦ç»Ÿè®¡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡æ‘˜è¦
        """
        total_reviews = sum(len(p.reviews) for p in papers)
        total_pros = sum(
            len(r.pros) for p in papers for r in p.reviews
        )
        total_cons = sum(
            len(r.cons) for p in papers for r in p.reviews
        )
        
        # ç»Ÿè®¡å„ç±»åˆ«çš„æ•°é‡
        category_stats = {cat: {"pros": 0, "cons": 0} for cat in Config.CATEGORIES}
        
        for paper in papers:
            for review in paper.reviews:
                for pro in review.pros:
                    cat = pro.get("category", "å…¶ä»–")
                    if cat in category_stats:
                        category_stats[cat]["pros"] += 1
                
                for con in review.cons:
                    cat = con.get("category", "å…¶ä»–")
                    if cat in category_stats:
                        category_stats[cat]["cons"] += 1
        
        summary = {
            "total_papers": len(papers),
            "total_reviews": total_reviews,
            "total_pros": total_pros,
            "total_cons": total_cons,
            "avg_pros_per_review": total_pros / total_reviews if total_reviews > 0 else 0,
            "avg_cons_per_review": total_cons / total_reviews if total_reviews > 0 else 0,
            "category_distribution": category_stats
        }
        
        return summary
    
    def display_extraction_summary(self, papers: List[Paper]):
        """æ˜¾ç¤ºç‰¹å¾æå–æ‘˜è¦"""
        summary = self.get_extraction_summary(papers)
        
        print("\n" + "=" * 50)
        print("ç‰¹å¾æå–æ‘˜è¦")
        print("=" * 50)
        print(f"å¤„ç†è®ºæ–‡æ•°: {summary['total_papers']}")
        print(f"å®¡ç¨¿è®°å½•æ•°: {summary['total_reviews']}")
        print(f"æå–ä¼˜ç‚¹æ€»æ•°: {summary['total_pros']}")
        print(f"æå–ç¼ºç‚¹æ€»æ•°: {summary['total_cons']}")
        print(f"å¹³å‡æ¯æ¡å®¡ç¨¿çš„ä¼˜ç‚¹æ•°: {summary['avg_pros_per_review']:.2f}")
        print(f"å¹³å‡æ¯æ¡å®¡ç¨¿çš„ç¼ºç‚¹æ•°: {summary['avg_cons_per_review']:.2f}")
        
        print("\nå„ç±»åˆ«åˆ†å¸ƒ:")
        for cat, counts in summary['category_distribution'].items():
            total = counts['pros'] + counts['cons']
            if total > 0:
                print(f"  {cat}:")
                print(f"    ä¼˜ç‚¹: {counts['pros']}, ç¼ºç‚¹: {counts['cons']}")
        
        print("=" * 50 + "\n")


if __name__ == "__main__":
    # æµ‹è¯•ç‰¹å¾æå–å™¨
    from data_loader import DataLoader, Paper, Review
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_review = Review(
        reviewer_id="test_reviewer",
        review_text="""
        This paper presents an interesting approach to neural machine translation.
        
        Strengths:
        - The proposed attention mechanism is novel and effective
        - Experimental results are comprehensive
        - The paper is well-written
        
        Weaknesses:
        - Missing comparisons with recent SOTA methods
        - The computational cost analysis is insufficient
        - Some implementation details are unclear
        """,
        actual_score=7.0
    )
    
    test_paper = Paper(
        paper_id="test_paper",
        title="Attention Is All You Need",
        abstract="We propose a new neural network architecture based on attention mechanisms."
    )
    test_paper.add_review(test_review)
    
    # æµ‹è¯•æå–
    extractor = FeatureExtractor()
    
    try:
        Config.validate()
        extractor.extract_from_paper(test_paper)
        
        print("\næå–ç»“æœ:")
        print(f"ä¼˜ç‚¹æ•°: {len(test_review.pros)}")
        print(f"ç¼ºç‚¹æ•°: {len(test_review.cons)}")
        
        # æµ‹è¯•ä¿å­˜
        extractor.save_extraction_results([test_paper])
        
        print("\nâœ“ ç‰¹å¾æå–å™¨æµ‹è¯•å®Œæˆï¼")
        
    except ValueError as e:
        print(f"\nâš  éœ€è¦é…ç½®APIå¯†é’¥æ‰èƒ½æµ‹è¯•: {e}")
