{
  "paper": {
    "id": "v27yHgKtMv",
    "forum": "v27yHgKtMv",
    "title": "Calibration of ordinal regression networks",
    "authors": "Daehwan Kim, Haejun Chung, Ikbeom Jang",
    "keywords": "Ordinal regression, Calibration, Deep neural networks, Unimodality, Loss function, Soft ordinal encoding, Label smoothing, Order-aware calibration",
    "abstract": "Recent studies have shown that deep neural networks are not well-calibrated and produce over-confident predictions.\nThe miscalibration issue primarily stems from the minimization of cross-entropy, which aims to align predicted softmax probabilities with one-hot labels. In ordinal regression tasks, this problem is compounded by an additional challenge: the expectation that softmax probabilities should exhibit unimodal distribution is not met with cross-entropy. Rather, the ordinal regression literature has focused on unimodality and overlooked calibration. To address these issues, we propose a novel loss function that introduces order-aware calibration, ensuring that prediction confidence adheres to ordinal relationships between classes. It incorporates soft ordinal encoding and label-smoothing-based regularization to enforce both calibration and unimodality. Extensive experiments across three popular ordinal regression benchmarks demonstrate that our approach achieves state-of-the-art calibration without compromising accuracy.",
    "pdf_link": "https://openreview.net/pdf/68595e0558e86c2f45ec3d79e27ab7eb06ef8403.pdf",
    "submission_date": "2024-09-28 10:18:27.757000"
  },
  "reviews": [
    {
      "note_id": "pyP8NcKL9B",
      "replyto": "v27yHgKtMv",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-05 17:04:13.903000",
      "modified": "2024-11-12 16:10:43.383000",
      "signatures": "ICLR.cc/2025/Conference/Submission14054/Reviewer_aBHc",
      "actor": "Reviewer aBHc",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 5,
      "confidence": 5,
      "soundness": 2,
      "presentation": 2,
      "contribution": 2,
      "strengths": "1. a novel regularization term is used to promote unimodularity.\n2. Paper is well written and easy to read.",
      "weaknesses": "1. Theoretical proofs of calibration and unimodularity are missing.",
      "questions": "1. Many things in the paper are not defined properly. Please clearly specify the definitions of calibration and modularity. \n\n2. Why MAE is not used as a metric in the experiments, as all the loss functions used are surrogates for it. It would be interesting to see comparison results on MAE.\n\n3. Please explain the regularization term in detail as it is still unclear how it promotes unimodularity. A graphical explanation will also help.\n\n4. A theoretical analysis of calibration and unimodularity of the proposed approach is missing. \n\n5. Is the proposed approach rank consistent? If not, then how frequently it violates ranking of thresholds. An experimental study on it will be helpful.\n\n6. How does the performance of the model vary  with the variation in thevalue of $t$ used in the regularization term.",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "In this paper, authors propose an approach for calibration of ordinal regression. They propose a loss function that introduces order-aware calibration They use soft ordinal encoding and label-smoothing-based regularization to enforce both calibration and unimodality. To show the efficiency of the proposed approach, authors propose extensive experimental results on benchmark datasets."
    },
    {
      "note_id": "iwd4iKIT2v",
      "replyto": "v27yHgKtMv",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-01 09:46:34.905000",
      "modified": "2024-11-12 16:10:43.381000",
      "signatures": "ICLR.cc/2025/Conference/Submission14054/Reviewer_xcrH",
      "actor": "Reviewer xcrH",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 5,
      "confidence": 4,
      "soundness": 3,
      "presentation": 2,
      "contribution": 2,
      "strengths": "1. The problem of calibration in the context of ordinal regression sounds novel and important. As far as I know, this work should be the first work to solve this issue.\n\n2. The improvement is significant empirically. From Table 2, we can observe a great improvement in the calibration of ordinal regression models and the classification accuracy is preserved.",
      "weaknesses": "1. The L_{REG} defined in Equation 2 is not clearly explained. In particular, the design of I(r) is hard to understand for readers. It would be better if the authors could elaborate on how the regularization is constructed.\n\n2. The writing of the gradient analysis in Subsection 3.4 is not clear.  The authors may need to improve the writing in this part, or it might be too challenging for readers to follow.\n\n3. The technical novelty of the proposed method is not presented. While the authors claim that the method considers the unimodality compared to current calibration methods and considers the calibration when compared to current Ordinal losses, I am not clear about if this method is newly designed in each aspect. In other words, the authors may need to show the new insight of calibration part compared to calibration methods.\n\ntypos:\n1. Line 49, Oridnal -> Ordinal.\n\nThe major issue of this work is on the writing: readers cannot easily understand why we should design such a regularization and how t works here. I will improve my score if the authors can make it clear in the revised version.",
      "questions": "1. In equation 3, we generally adopt a hyperparameter $\\lambda$ to balance the loss and the regularization, like $L = L_1 + \\lambda L_2$. Could you explain why this method does not require such a hyperparameter? And why $t$ can control the strength of this regularization?\n\n2. Could you explain why SCE and ACE are improved a lot, but ECE is not?",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "In this paper, the authors aim to enhance the confidence calibration of ordinal regression in the training stage. The main challenge of this task is to consider the calibration and unimodality together. To address this challenge, they propose a new loss function for ordinal regression, which combines order-aware calibration with a unimodal regularization term (based on the SORD encoding). In particular, their method enforces both calibration and unimodality by explicitly modeling the ordinal relationships between classes. The effectiveness of their method is validated on three public datasets."
    },
    {
      "note_id": "EVFTRrkL9I",
      "replyto": "v27yHgKtMv",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-10-31 05:13:44.434000",
      "modified": "2024-11-12 16:10:43.347000",
      "signatures": "ICLR.cc/2025/Conference/Submission14054/Reviewer_vAiU",
      "actor": "Reviewer vAiU",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 3,
      "confidence": 4,
      "soundness": 2,
      "presentation": 2,
      "contribution": 2,
      "strengths": "•\tThe motivation behind the new method is well-articulated, clearly highlighting the limitations of traditional cross-entropy (CE) loss in ordinal tasks and the miscalibration in modern ordinal regression models.\n•\tThe proposed method is straightforward, and easy to implement.",
      "weaknesses": "1.\tIt is unclear how the regularization component of ORCU promotes calibration. Lines 240-252 discuss scenarios where the model is under or overconfident, yet this confidence is based on a soft-encoded distribution not directly related to the data, which raises questions about its reflection of \"real\" confidence. Additionally, I would appreciate a more rigorous explanation of how this regularization approach aligns with the standard mathematical definition of calibration. Could the authors provide a clearer mathematical justification for this relationship?\n\n2.\tThe paper claims that CE loss leads to overconfident predictions, yet the reliability diagrams presented indicate underconfident outcomes in the experiments, seemingly contradicting this claim. Temperature scaling, a prominent calibration technique, relies on CE, further challenging the assertion that CE is fundamentally flawed for calibration. Could the authors address this discrepancy and clarify why their results show underconfidence in CE where overconfidence might be expected? Additionally, a nuanced discussion of CE’s strengths and limitations for calibration, especially in light of techniques like temperature scaling, would be valuable.\n\n3.\tThe evaluation is limited to loss function baselines. Including additional non-loss-based methods for ordinal regression, such as the approach presented in https://arxiv.org/pdf/2303.04547 for unimodality could highlight the unique benefits of ORCU more effectively. I recommend incorporating a discussion on why ORCU and loss function-based methods may offer advantages over such approaches.\n\n4.\tThe experiments were conducted on only three datasets, which limits the scope for evaluating the method’s robustness across a wider range of ordinal regression tasks. Incorporating a more extensive dataset selection would allow for a better assessment of the generalizability of the approach.",
      "questions": "See weaknesses.",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "The proposed method introduces ORCU, a novel loss function designed to ensure calibration and unimodality in ordinal regression tasks. ORCU leverages soft ordinal encoding and order-aware regularization to produce calibrated and unimodal probability distributions, which are particularly valuable in high-stakes applications requiring reliable confidence estimates and accurate predictions."
    },
    {
      "note_id": "46aPUZNSQy",
      "replyto": "v27yHgKtMv",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-10-30 07:53:48.794000",
      "modified": "2024-11-12 16:10:43.355000",
      "signatures": "ICLR.cc/2025/Conference/Submission14054/Reviewer_kYWf",
      "actor": "Reviewer kYWf",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 3,
      "confidence": 5,
      "soundness": 1,
      "presentation": 1,
      "contribution": 2,
      "strengths": "- This work addresses an important overconfidence issue in ordinal regression tasks\n- The proposed loss function is assumed to address both accuracy and confidence of the cross entropy loss based model during optimization without additional post-training calibration\n- The authors justify the unimodality enforcement of the proposed loss by gradient analysis",
      "weaknesses": "**Major**:\n- The main focus of the work is CE-loss based ordinal regression which is not an optimal loss for this task and  several methods were proposed without CE loss:  [1-4]\n- The motivation in Sec 3.1 is unclear, how the calibration is defined and why it is not implied by CE-loss. The discussion seems to be valid for the ordered nature of classes but not for calibration. It is better to discuss the motivation for each problem separately.\n - $\\mathcal{L}_{SCE}$ - the explanation in L175-177 is unclear, how the defined loss encourages what the authors claim - maybe it is explained by Diaz et. al but the manuscript should be self contained with additional clarification. It is also not clear how it helps to reflect the ordinal relationships.\n- The Sec. 3.3 in unclear, the explanation and derivation of the loss formula should come before presenting the loss term \n  - why the authors choose it\n  - how it helps to ensure calibration\n  - what is r ?\n  - The L181-183 is unclear.\n\n- Weak evaluation with only 3 small-sample datasets - overall the improvement is incremental so presenting results on more datasets could be beneficial.\n- Missing unimodality evaluation - the authors claim the model enforces unimodality - please show it in the results as in [5]\n- Missing additional deep ordinal regression baselines [1-5].\n- Sec. 3.4 - while it is clear why the loss term enforces unimodality, I’m not sure how it enforces calibration. By saying “*by increasing the gradient for such incorrect predictions, the model is able to reduce the predicted probability for the incorrect class more effectively*” you can claim the same for the standard CE loss. \n- while it could be seen from the results that calibration metrics improved, I’m not sure it is clear from the manuscript why it works.\n\n**Minor**:\n- Missing additional deep ordinal regression methods in the related work discussion\n- It is better to put Figure 1 closer to the gradient analysis section to make it easier to follow\n\nReferences:\n[1] Liu, X., et al. (2019a). Unimodal-uniform constrained wasserstein training for medical diagnosis. In Proceedings of the IEEE International Conference on Computer Vision Workshops\n\n[2] Beckham, C. et al.  (2017). Unimodal probability distributions for deep ordinal classification.\n\n[3] Wenzhi Cao et. al (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation. Pattern Recognition\n\n[4] Xintong Shi et al. (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities.\n\n[5] Cardoso, J. S. et. al (2023). Unimodal distributions for ordinal regression",
      "questions": "- Please address the points I raised in the weaknesses part\n- The methods that are not based on CE loss - like optimal transport loss - how are the limitations applied to them? \n- Missing definition of  z_{n.k} and intermediate step to get f’=y - p, also r is not defined before it presented \n- Can you elaborate please why the gradient encourages the model to distribute probability across adjacent classes ( as you claim in the sentence in Line 176) compared to the statement in Lines 147-149",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "The authors proposed an addition to the cross entropy loss term that  corrects overconfident predictions for incorrect labels. The advantage of the proposed loss is that the calibration is done jointly with the accurate prediction learning during the training optimization and doesn’t require additional post-training steps. The method was evaluated on the 3 datasets and compared against CE-based ordinal models and calibration-loss based and showed an improvement in calibration/accuracy metrics."
    }
  ]
}
