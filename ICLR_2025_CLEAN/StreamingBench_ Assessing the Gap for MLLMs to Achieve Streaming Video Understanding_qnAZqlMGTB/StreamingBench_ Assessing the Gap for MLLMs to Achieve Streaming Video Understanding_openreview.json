{
  "paper": {
    "id": "qnAZqlMGTB",
    "forum": "qnAZqlMGTB",
    "title": "StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding",
    "authors": "Junming Lin, Zheng Fang, Zihao Wan, Fuwen Luo, Chi Chen, Peng Li, Yang Liu, Maosong Sun",
    "keywords": "Benchmark, Streaming Video Understanding, Multimodal Large Language Models, Video Benchmark, Evaluation",
    "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has expanded their capabilities from image comprehension to video understanding. However, most of these MLLMs focus primarily on ofﬂine video comprehension, necessitating extensive processing of all video frames before any queries can be made. This presents a signiﬁcant gap compared to the human ability to watch, listen, think, and respond to streaming inputs in real time, highlighting the limitations of current MLLMs. In this paper, we introduce StreamingBench, the ﬁrst comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs. StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding and (3) contextual understanding. The benchmark consists of 18 tasks, featuring 900 videos and 4,500 human-curated QA pairs. Each video features ﬁve questions presented at different time points to simulate a continuous streaming scenario. We conduct experiments on StreamingBench with 15 open-source and proprietary MLLMs and ﬁnd that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and GPT-4o perform signiﬁcantly below human-level streaming video understanding capabilities. We hope our work can facilitate further advancements for MLLMs, empowering them to approach human-level video comprehension and interaction in more realistic scenarios.",
    "pdf_link": "https://openreview.net/pdf/510ba84e9bb94cbbd1bcb5060d0bf89b4703ddb7.pdf",
    "submission_date": "2024-09-28 11:43:43.128000"
  },
  "reviews": [
    {
      "note_id": "z3AD6FocQ6",
      "replyto": "qnAZqlMGTB",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-05 09:23:01.333000",
      "modified": "2024-11-12 16:13:19.979000",
      "signatures": "ICLR.cc/2025/Conference/Submission14232/Reviewer_7TRD",
      "actor": "Reviewer 7TRD",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 3,
      "confidence": 4,
      "soundness": 2,
      "presentation": 3,
      "contribution": 2,
      "strengths": "- This work introduces a new benchmark designed to evaluate video models in streaming scenarios.\n- It conducts insightful experiments, such as \"Does Redundant Information Affect Contextual Understanding?\", which provide valuable perspectives in this area.",
      "weaknesses": "- Although this benchmark focuses on streaming scenarios, a standard video LLM can handle it effectively with simple preprocessing. For instance, whenever a question arises, the model can process all frames up to that timestamp. With this approach, the benchmark may not differ significantly from traditional video benchmarks. Therefore, it is essential for this benchmark to identify scenarios that cannot be simplified to an offline setting.\n\n- While handling redundant information is indeed critical for video LLMs, this challenge is not exclusive to streaming scenarios; it is a general issue for any long-video task. As a result, the insights from this paper may be overshadowed by findings from benchmarks specifically focused on long-video understanding.\n\n- The annotation process lacks clarity. Specifically, how do human annotators manually label QA pairs for omni-source understanding and other contextual understanding tasks? What measures are in place to ensure the quality of each question, and what specific strategies were employed?",
      "questions": "As mentioned in the weakness",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "This work proposes a benchmark called StreamingBench to evaluate video LLM capabilities in streaming settings. StreamingBench introduces several tasks tailored to streaming scenarios, including real-time visual understanding, omni-source understanding, and contextual understanding."
    },
    {
      "note_id": "65oPPmAj1N",
      "replyto": "qnAZqlMGTB",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-04 04:20:12.098000",
      "modified": "2024-11-12 16:13:19.965000",
      "signatures": "ICLR.cc/2025/Conference/Submission14232/Reviewer_szQS",
      "actor": "Reviewer szQS",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 6,
      "confidence": 5,
      "soundness": 3,
      "presentation": 3,
      "contribution": 3,
      "strengths": "- StreamingBench addresses a relatively unexplored area in MLLM research—real-time video understanding. By allowing questions to be asked at any moment and incorporating both audio and visual data, it expands the scope of existing benchmarks.",
      "weaknesses": "- The methodology for collecting 900 videos from YouTube lacks sufficient detail.\n- Given that the study focuses on a model's capability that is seldom addressed—real-time video understanding—it would be beneficial to create or curate a specific supervised fine-tuning (SFT) dataset. This would allow for an evaluation of model performance post-SFT.\n- There is a lack of exploration into the model’s ability to generate proactive outputs. Designing a corresponding SFT dataset to assess whether the model performs better with prior exposure to similar outputs would provide valuable insights.\n- Clarification is needed on how open-source models like Qwen2-VL tackle omni-source understanding problems, particularly in the absence of audio inputs. This comparison could shed light on the robustness of the proposed benchmark.",
      "questions": "See Weaknesses",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "This paper introduces StreamingBench, a benchmark designed to evaluate the capabilities of MLLMs in understanding online streaming videos. Key features of StreamingBench include the ability to pose questions at any point during the video, rather than requiring the full video to be viewed first. The benchmark also considers both visual and audio inputs, and it takes into account the influence of historical interactions in multi-turn dialogues."
    },
    {
      "note_id": "nu0p7u8bvt",
      "replyto": "qnAZqlMGTB",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-03 15:38:47.315000",
      "modified": "2024-11-12 16:13:19.879000",
      "signatures": "ICLR.cc/2025/Conference/Submission14232/Reviewer_wiNw",
      "actor": "Reviewer wiNw",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 6,
      "confidence": 5,
      "soundness": 4,
      "presentation": 3,
      "contribution": 3,
      "strengths": "1. It is the first valid benchmark on streaming long videos. The questions are designed properly to reflect the information gained in a streaming long video, and highly resembles what human will ask when continuously watching a video.\n2. The evaluation and discussion are both very solid.\n3. Human performance is another plus.",
      "weaknesses": "1. The real-time understanding part is nice, but seems a little bit trivial. From all kinds of NIAH evaluations, all models can best answer questions near the ending part of the input, and questions related to \"current moments\" (which is actually the ending part of input as implemented) might not be so important. Would love to see the understanding on \"remembering earlier moments\" and the discrepancy from \"current moments\" for LMMs.\n\n2. The omni-source (visual+audio) part is good. However, how are LMMs without audio abilities evaluated? As these `audio'-related questions seem to be mostly about speeches, do authors plan to interleave text ASR into the model for evaluation? At present, sadly we only see a black-box Gemini-1.5-Pro (for which we do not know how they integrate audio and video) being evaluated with audio.\n\n3. A minor suggestion: the omnisource part of the benchmark is related to \"referring reasoning\" part of LongVideoBench, an interleaved benchmark for frames and ASR texts, which also needs to judge between concurrent video and audio information in a video. As some other long video benchmarks discussed in Tab 1, please also try to discuss it in the revised paper.",
      "questions": "Please see the weaknesses.",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "The paper introduces StreamingBench, a benchmark designed to evaluate the streaming video understanding abilities of Multimodal Large Language Models (MLLMs). Traditional MLLMs are effective in offline video comprehension but struggle with real-time, streaming scenarios that require instant processing, synchronizing visual and audio inputs, and understanding context over time. StreamingBench addresses this by presenting 900 videos across diverse real-world scenarios, structured into 18 tasks and 4,300 human-curated question-answer pairs. These tasks test MLLMs on real-time visual, omni-source, and contextual understanding, aiming to bridge the gap between MLLMs and human-level comprehension in streaming contexts. Testing 13 MLLMs, including state-of-the-art proprietary models, revealed significant limitations in current models, especially in omni-source and contextual tasks, suggesting that MLLMs need further development to match human performance in real-time understanding. In general, it is a solid paper and I would recommend acceptance to it."
    },
    {
      "note_id": "RDh4IvnXCP",
      "replyto": "qnAZqlMGTB",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-03 03:31:41.573000",
      "modified": "2024-11-12 16:13:19.779000",
      "signatures": "ICLR.cc/2025/Conference/Submission14232/Reviewer_Rih2",
      "actor": "Reviewer Rih2",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 8,
      "confidence": 4,
      "soundness": 4,
      "presentation": 4,
      "contribution": 3,
      "strengths": "S1: In this paper, the authors propose a benchmark to evaluate the MLLMs' capabilities of streaming video understanding, which is novel and unexplored previously. I believe this will facilitate the advancement of Video MLLMs.\n\nS2: The benchmark considers both video and audio modalities, which have been absent in most previous benchmarks.\n\nS3: The experiments and analysis are comprehensive and detailed, effectively highlighting the limitations of current Video MLLMs in understanding streaming video.\n\nS4: The writing is clear and well-structured.",
      "weaknesses": "W1: The impact of language model size on performance has not been analyzed. For instance, models like InternVL-V2 come in 1B, 2B, 4B, 8B, 26B, 40B, and 72B parameter versions, while Video-LLaMA2, LLaVA-OneVision, and Qwen2-VL also have 72B versions. Expanding your experiments to include these variations and providing a more detailed analysis would enhance your work. Additionally, exploring the number of frames the model can process would offer valuable insights.\n\nW2: Several significant models are missing from the evaluation, such as LongVILA [1], Long-LLaVA [2], and Oryx [3]. Including these would provide a more comprehensive comparison.\n\n[1] https://github.com/NVlabs/VILA/blob/main/LongVILA.md\n\n[2] https://github.com/FreedomIntelligence/LongLLaVA\n\n[3] https://github.com/Oryx-mllm/Oryx",
      "questions": "Q1: Do you plan to extend your benchmark to support additional streaming video understanding tasks, such as dense streaming video captioning or grounding? Currently, it only supports QA.\n\nQ2: Could you provide more details on how audio is utilized in the MLLMs? I am aware that Video-LLaMA2 supports audio input, but what about the other models?\n\nQ3: Have you considered memory constraints?  I believe this is a crucial factor in streaming video understanding. It would be more consistent and fair to have a fixed memory limit applied across all models, as retaining all input frames throughout a task—particularly in real-world applications—may not be feasible. If some models retain all frames, it could lead to an unfair advantage.",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "The authors propose a new benchmark, StreamingBench, for evaluating MLLMs in streaming video understanding. It assesses three aspects of streaming video understanding: real-time visual understanding, omni-source understanding, and contextual understanding. There are 18 tasks in total. They evaluate 13 open-source Video MLLMs and 3 proprietary MLLMs on this benchmark and analyze the results."
    }
  ]
}
