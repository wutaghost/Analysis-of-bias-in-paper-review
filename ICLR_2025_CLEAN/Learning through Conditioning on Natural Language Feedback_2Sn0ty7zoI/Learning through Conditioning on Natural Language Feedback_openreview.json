{
  "paper": {
    "id": "2Sn0ty7zoI",
    "forum": "2Sn0ty7zoI",
    "title": "Learning through Conditioning on Natural Language Feedback",
    "authors": "Dylan Hillier, Cheston Tan, Jing Jiang",
    "keywords": "Social Learning, Natural Language Feedback, Instructive Learning",
    "abstract": "In this paper we explore the simple idea of teaching models by allowing them to condition their answers on natural language feedback. Motivated by the idea that natural language interactions provide a targeted, flexible, and level-appropriate reward signal, we study the ability of small instruction-tuned models to leverage feedback from a larger frontier model. We find while the frontier model provides generally high quality feedback, especially smaller models can struggle to use this due to noise in their generative output. After incorporating techniques like negative sampling, we find that models trained on these feedback-conditioned responses can perform similarly to those trained directly on teacher responses. We explore training using supervised finetuning and preference learning algorithms over a broad set of tasks including Big-Bench Hard. These findings are broadly applicable and our methods rely only on the ability of models to give and receive linguistic feedback. As such, they contribute to a growing body of work exploring how to best utilise the linguistic capabilities of language models for human-like instructive learning.",
    "pdf_link": "https://openreview.net/pdf/602938364839100b831b463126306b129a3e6944.pdf",
    "submission_date": "2024-09-28 11:43:04.298000"
  },
  "reviews": []
}
