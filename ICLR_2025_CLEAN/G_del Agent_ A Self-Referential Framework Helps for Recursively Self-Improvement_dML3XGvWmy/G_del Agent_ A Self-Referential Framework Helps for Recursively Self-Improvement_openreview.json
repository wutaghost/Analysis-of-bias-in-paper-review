{
  "paper": {
    "id": "dML3XGvWmy",
    "forum": "dML3XGvWmy",
    "title": "Gödel Agent: A Self-Referential Framework Helps for Recursively Self-Improvement",
    "authors": "Xunjian Yin, Xinyi Wang, Liangming Pan, Xiaojun Wan, William Yang Wang",
    "keywords": "Agent, Large Language Model, Reasoning, Self-Improvement",
    "abstract": "The rapid advancement of large language models (LLMs) has significantly enhanced the capabilities of AI-driven agents across various tasks. However, existing agentic systems, whether based on fixed pipeline algorithms or pre-defined meta-learning frameworks, cannot search the whole agent design space due to the restriction of human-designed components, and thus might miss the globally optimal agent design. In this paper, we introduce Gödel Agent, a self-evolving framework inspired by the Gödel machine, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms. Gödel Agent leverages LLMs to dynamically modify its own logic and behavior, guided solely by high-level objectives through prompting. Experimental results on mathematical reasoning and complex agent tasks demonstrate that implementation of Gödel Agent can achieve continuous self-improvement, surpassing manually crafted agents in performance, efficiency, and generalizability.",
    "pdf_link": "https://openreview.net/pdf/7ed11d3c60fd89a6aef8488d9f6d42d1eb54dd43.pdf",
    "submission_date": "2024-09-28 09:39:39.458000"
  },
  "reviews": [
    {
      "note_id": "vRPVGJuHo9",
      "replyto": "dML3XGvWmy",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-04 16:25:47.609000",
      "modified": "2024-11-22 18:46:32.350000",
      "signatures": "ICLR.cc/2025/Conference/Submission13979/Reviewer_ALKV",
      "actor": "Reviewer ALKV",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 5,
      "confidence": 3,
      "soundness": 1,
      "presentation": 2,
      "contribution": 2,
      "strengths": "* The proposed method is conceptually simple and clear and provides and intriguing possibility of building self-improving agents with current LLM technology.\n* The experiments include multiple benchmarks, a number of baselines and generally suggest strong results.\n  * The authors run ablations of different components of their agent and find all of them improve performance.\n* The qualitative analysis of the results is insightful and provides a good understanding of the strategies the self-improving agent implements.",
      "weaknesses": "**Results**\nI'm concerned about the presentation of the experiments and the **fairness of the empirical evaluation**. My key concerns are that the Godel agent uses significantly more inference compute than any of the other methods and that the description of the evaluation protocol is not sufficient to determine if the evalaution is fair.\n\nHere is my understanding of the evaluation setup (please correct me if I'm wrong):\n* The Godel agent is run for N iterations on a validation set of samples. In each iteration is produces a new policy and a new learning function.\n* After N iteration the policy is evaluated on held-out test problems and this result is reported.\n\nThis process gives the Godel agent significantly more inference compute than any of the hand-designed agent systems, which makes the comparison unfair. The only fair comparison in Table 1 is to Meta Agent Search which seems to typically be comparable in performance. Based on these results, I am not convinced the the Godel agent actually improves performance in a fair comparison.\n\nThis concern could be addressed by letting the baseline be a best-of-N method that applies N somewhat random perturbations to the policy and chooses the best of them. For example, this could be implemented by using GPT-4 to generate N different prompt variations and choose the best according to performance on the validation data.\n\nRelatedly, according to Appendix B the Godel agent uses GPT-4 for the learning algorithm but all methods only use GPT-3.5 for the policy. This makes the comparison additionally skewed in favor of the Godel agent which has access to GPT-4 in contrast to the other methods. A fair comparison would use GPT-4 for both the learning algorithm and the policy.\n\n\n**Presentation**\n\nI also have some concerns about the presentation and framing of self-improving agents in the paper. The paper often uses the term \"self-awareness\" without qualification. I'm concerned that anthropomorphization leads to a less scientific discussions and I would recommend the authors try to rewrite the paper to remove phrases like \"Our Godel Agent achieves self-awareness\".\n\nMoreover, potential risks from self-improving AI are not appropriately discussed. There is a significant literature on risks from self-improving AIs and this is a key concern in the AGI safety community. I think this literature should be acknowledged in the paper and there should be a more complete discussion of broader impacts of this technique than currently the case.",
      "questions": "**Clarification questions**: Please answer the following questions to help me better evaluate the paper.\n\n* Does the learning agent ever see the data the final policy will be evaluated on?\n* For how many iterations do you run the Godel agent and how did you determine this hyperparameter?\n* In general, how did you tune prompt variations and hyperparameters for this agent? How did you prevent overfitting to the test data?\n* Could you please clarify the action space of the agent and how the agent learns about the possible interactions? Is it only given the prompt in Appendix A or is it also given few-shot samples for example?\n* How does the agent execute the policy? Is this happening automatically each iteration, or does the agent manually chose to exit the policy? Is this different during iteration and during evaluation?\n\n**Concerns**: Addressing the following concerns would make me reconsider my score.\n* Clarify the evaluation protocol.\n* Justify the choice of having baselines that use less inference compute. Provide a best-of-n baseline or argue why this is not a good comparison.\n* Clarify the comparison to Meta Agent Search and wheather your method produces qualitatively better solutions.\n* Justify the choice to use GPT-3.5 for the policy models. If possible, provide some results using GPT-4 for the policies.\n\n**Overall assessment**\nI'm intrigued by the premise of the paper, but quite concerned about the evaluation and reproducibility. If the authors can address my concerns and answer my questions, I'm willing to reconsider my score.",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "The paper proposes a self-improving language model agent (\"Godel agent\") and evaluates it on a variety of benchmarks. The results suggest that the self-improving agent achieves better performance than hand-designed agent scaffolds or meta-learned agents, while also being cheaper to run than the latter."
    },
    {
      "note_id": "TiuKFU2CPe",
      "replyto": "dML3XGvWmy",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-04 04:53:23.092000",
      "modified": "2024-11-12 16:21:27.484000",
      "signatures": "ICLR.cc/2025/Conference/Submission13979/Reviewer_waxH",
      "actor": "Reviewer waxH",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 6,
      "confidence": 3,
      "soundness": 3,
      "presentation": 3,
      "contribution": 3,
      "strengths": "* Seems like a promising and open-ended approach for removing humans in the loop of agent pipeline building\n* ADAS Meta Agent is a strong similar baseline and Godel outperforms it\n* Demonstrated effective self-improvement across multiple domains including reasoning",
      "weaknesses": "* Experiments seem limited in scope—mostly on controlled tasks; might not scale to complex real-world applications or embodied tasks\n* The final policies that are returned by the method don't seem very complicated or different from a basic human designed template. The agent designs discovered in the ADAS paper seem much more complex and creative. What explains this difference? \n* Given the limited evaluations, the room for self-improvement seems limited, can we really get much better than the base model for mathematical reasoning with fancy agent pipelines and prompts? A better showcase of the method would be on openended or embodied tasks vs text based reasoning\n* What’s the upper limit of self improvement, does it saturate?\n   *Whats the improvement in the 6 iterations\n* More detailed comparison to ADAS is needed in related work (Automated Design of Agentic Systems)\n   * What is Meta Agent Search and why does this Godel agent perform better?\n      * This needs to be clearer in the paper\n* The method is somewhat vague and not clear what’s going on, and what this has to do with self reference/recursion. Is this different than just tasking an LLM with modifying its own agent code?",
      "questions": "How does Gödel Agent ensure safety and prevent harmful behaviors during self-modification? How do you put constraints on it such that it doesn't blow all of your GPT credits or hog all the GPUs on your system?\n\nWhat mechanisms are in place to handle errors or prevent the agent from degrading its performance over time?\n\nCan this approach scale to more complex tasks that require long-term planning or interaction with unpredictable environments?\n\nHow does the agent's performance depend on the underlying LLM's capabilities? Can it surpass those limitations?",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "* A big limitation of LLM agents is that there are handcrafted, self-improving agents are a promising direction to make this more autonomous\n* self referential framework that enables agents to recursively improve themselves without predefined routines\n* Agent can alter its own code and runtime memroy\n\nThe paper introduces Gödel Agent, a self-referential framework that enables agents to recursively improve themselves without predefined routines or fixed optimization algorithms.\nInspired by the Gödel machine, Gödel Agent allows agents to modify their own code and logic using large language models (LLMs), guided only by high-level objectives.\nThe agent uses techniques like monkey patching to read and alter its runtime memory, achieving self-awareness and self-modification.\nExperiments across coding, science, and math tasks show that Gödel Agent outperforms manually crafted agents in performance, efficiency, and generalizability."
    },
    {
      "note_id": "INBceHz3F2",
      "replyto": "dML3XGvWmy",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-01 20:30:19.326000",
      "modified": "2024-11-25 05:29:10.487000",
      "signatures": "ICLR.cc/2025/Conference/Submission13979/Reviewer_xqBT",
      "actor": "Reviewer xqBT",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 6,
      "confidence": 5,
      "soundness": 3,
      "presentation": 3,
      "contribution": 2,
      "strengths": "1. The workflow proposed in this paper is interesting; the core idea of recursive improvement is not implemented by improving the response itself step-by-step, but through updating the policy in an indirect way.\n\n2. The paper is well written, and easy to follow.\n\n3. The illustrations in figure 1 and examples given in Appendix C are clear and informative.",
      "weaknesses": "1. The motivation for adding four additional tools is not well-justified; there is no evidence supporting that these are the correct tools to add or that they comprehensively cover all needs. It's counterintuitive because if we claim the model has the ability to refine its policy and develop new policies, why do we need to provide tool guidance? Additionally, it's notable in Appendix C that although there is variability in the best policy, this policy seems can be decomposed into a combination of the listed tools, which suggests this work is not fully distinguished from meta-learning optimized agents where humans manually define the high-level strategy, and the agent learn a strategy to combine or rank the high-level strategy.\n\n2. The current framework heavily relies on the model's capability to generate and refine policies, as well as generate responses given a policy and query. However, this would not improve the model's ability to acquire new knowledge if such kind of off-line dataset available.\n\n3. The current refinement requires an oracle to progress the performance (U in line 6 from Algorithm 1). For the current Table 1, is the number of oracle utility function calls the same across all methods, and is the number of model queries the same for different settings?",
      "questions": "1. Based on Appendix C, the best policy found by Godel Agent for MMLU is the same as CoT-SC, why are the two numbers reported in Table 1 different? Similarly for the best policy for GPQA, why is it different from CoT? If this is due to extended few-shot examples or specific instructions, then it seems more like a prompt engineering problem, suggesting we're not using optimal prompts.\n\n2. The reason why the method is evaluated on MGSM instead GSM8K is not well-justified, for my understanding, multi-lingual is not a target of the framework. Moreover, all current benchmark, even regular CoT has high accuracy, this together with limitation, I am questioning the generalizability of the framework on harder tasks, like MATH (https://github.com/hendrycks/math), it is reported that MATH data is contanmintated in some training dataset, so it would be safer to evaluate on MATH500 (https://huggingface.co/datasets/qq8933/MATH500), and omni-MATH (https://omni-math.github.io/). Additionally, the current evaluation would benefit from other agentic tasks, including code generation tasks, like SWEBench (https://www.swebench.com/).\n\n3. For the related work, some recent recursive self-improvement work is not discussed\n[1] Havrilla, Alex, et al. \"Glore: When, where, and how to improve llm reasoning via global and local refinements.\" arXiv preprint arXiv:2402.10963 (2024).\n[2] Hosseini, Arian, et al. \"V-star: Training verifiers for self-taught reasoners.\" arXiv preprint arXiv:2402.06457 (2024).\n[3] Qu, Yuxiao, et al. \"Recursive introspection: Teaching language model agents how to self-improve.\" arXiv preprint arXiv:2407.18219 (2024).\n[4] Kumar, Aviral, et al. \"Training language models to self-correct via reinforcement learning.\" arXiv preprint arXiv:2409.12917 (2024).\n\n4. How to safeguard the current workflow if the model is allowed to modify the policy and interaction code directly, and the policy will be used to generate further responses. How can we prevent runtime memory issues and ensure that both the code implementation and the final generation are not harmful?",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "This paper proposes a new self-evolving framework, Godel Agent, which leverages LLMs to dynamically modify self-generated logic and behavior. Empirical experiments show that this framework enables agents to recursively improve themselves without predefined routines or fixed optimization algorithms."
    },
    {
      "note_id": "kvUPkaZQcZ",
      "replyto": "dML3XGvWmy",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-11-01 15:48:29.353000",
      "modified": "2024-11-30 04:55:42.272000",
      "signatures": "ICLR.cc/2025/Conference/Submission13979/Reviewer_7jQx",
      "actor": "Reviewer 7jQx",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 6,
      "confidence": 4,
      "soundness": 2,
      "presentation": 3,
      "contribution": 2,
      "strengths": "1) A framework\nThe authors propose a self-evolving framework, Gödel Agent, inspired by the Gödel machine, to enable recursive self-improvements of agents, without relying on predefined routines or fixed optimization algorithms.\n\n2) Experiments\nThe authors conduct experiments to validate the proposed approach.",
      "weaknesses": "1. Why LLM-based agent is a valid approach? No LLM is perfect. All LLMs make mistakes. What is reliable information? What is ground truth? Then why should we rely on LLMs to make decisions? For software engineering, no LLM can achieve a pass@1 of 100%. To make it worse, the popular evaluation method with several tests, is not perfect, so the evaluation results are not fully reliable.\n\n2.\nHow to make LLM-based agent a valid approach? LLMs may help generate plausible solutions. But there should be a mechanism to filter out or fix incorrect solutions.\n\nThe current submission does not provide such a mechanism.\n\nHow to improve an LLM-based agent? \nIt seems this boils down to waiting for stronger LLMs, which is likely out of the scope of the paper, though.\n\n3.\nThere are papers discussing / criticizing the reasoning / planning capacity of LLMs, which is the foundation of LLM-based agent (including those based on the Gödel machine), e.g.\n\nLLMs can’t plan, but can help planning in LLM-modulo frameworks, in ICML, 2024.\n\nGSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. arXiv 2024\n\nWithout sound capabilities for reasoning / planning, how to build LLM-based agents, in particular, the Gödel Agent?\n\nIt appears that empirical improvements are not enough to answer the above questions.",
      "questions": "see weaknesses",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "The authors propose a self-evolving framework, Gödel Agent, inspired by the Gödel machine, to enable recursive self-improvements of agents, without relying on predefined routines or fixed optimization algorithms.\nThe authors conduct experiments to validate the proposed approach."
    },
    {
      "note_id": "l8Mm0J42fj",
      "replyto": "dML3XGvWmy",
      "invitation": "",
      "note_type": null,
      "decision_label": null,
      "created": "2024-10-28 15:29:02.594000",
      "modified": "2024-11-24 10:29:12.441000",
      "signatures": "ICLR.cc/2025/Conference/Submission13979/Reviewer_J1mU",
      "actor": "Reviewer J1mU",
      "readers": "everyone",
      "title": null,
      "rating_or_recommendation": 5,
      "confidence": 4,
      "soundness": 2,
      "presentation": 2,
      "contribution": 2,
      "strengths": "The research topic is interesting and important. The paper provides an extensive evaluation.",
      "weaknesses": "1. There are several ambitious claims within the paper; for example, the abstract suggests that the method can explore the entire space to find the optimal solution. This is a bold statement and does not seem to be substantiated by the findings presented.\n2. The central claim about the capability to create \"different agents\" from an initial agent mostly involves modifying the conditioning of the autoregressive order through prompt engineering.\n3. Figure 1 shows some learnable components, yet the framework itself does not appear to incorporate learnable elements.\n4. Line 50 claims \"to eliminate the human design prior,\" which is challenging given that LLMs inherently contain human priors from being trained on human-generated text.\n5. Discussions on self-awareness of LLMs are speculative and might be better left out of a scholarly paper focused on introducing a new method.\n6. Section 3 is notably small and lacks clarity on the method’s implementation. It would benefit from additional details, particularly on how self-improvement is enacted within the Godel Agent and how evaluations are conducted.\n7. Lines 215-233 imply that evaluations rely on the environments provided. If this involves using the test set for evaluation, it represents a significant limitation of the method.\n8. How the solutions in Godel Agent are evaluated (referenced in line 6 of the pseudocode) needs clearer explanation.\n9. It would be beneficial to compare the cost of the Godel Agent with other methods, such as the CoT, both in terms of time and financial resources.\nThe authors might want to reevaluate their claims and revise the paper to provide a clearer presentation of the proposed method.",
      "questions": "NA",
      "ethics_flag": [
        "No ethics review needed."
      ],
      "code_of_conduct": "Yes",
      "body": "The paper presents the Godel Agent, a prompt engineering framework inspired by the Godel machine, claiming to be a self-improvement framework. The authors evaluate Godel Agent in 4 tasks, and the game of 24."
    }
  ]
}
